{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seonhp/miniconda3/envs/ucc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# module import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from scipy.stats import norm\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from scipy.stats import ttest_ind\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소유격 변경을 위한 변수\n",
    "\n",
    "contractions = {\"'cause\": 'because',\n",
    " \"I'd\": 'I would',\n",
    " \"I'd've\": 'I would have',\n",
    " \"I'll\": 'I will',\n",
    " \"I'll've\": 'I will have',\n",
    " \"I'm\": 'I am',\n",
    " \"I've\": 'I have',\n",
    " \"ain't\": 'is not',\n",
    " \"aren't\": 'are not',\n",
    " \"can't\": 'cannot',\n",
    " \"could've\": 'could have',\n",
    " \"couldn't\": 'could not',\n",
    " \"didn't\": 'did not',\n",
    " \"doesn't\": 'does not',\n",
    " \"don't\": 'do not',\n",
    " \"hadn't\": 'had not',\n",
    " \"hasn't\": 'has not',\n",
    " \"haven't\": 'have not',\n",
    " \"he'd\": 'he would',\n",
    " \"he'll\": 'he will',\n",
    " \"he's\": 'he is',\n",
    " \"here's\": 'here is',\n",
    " \"how'd\": 'how did',\n",
    " \"how'd'y\": 'how do you',\n",
    " \"how'll\": 'how will',\n",
    " \"how's\": 'how is',\n",
    " \"i'd\": 'i would',\n",
    " \"i'd've\": 'i would have',\n",
    " \"i'll\": 'i will',\n",
    " \"i'll've\": 'i will have',\n",
    " \"i'm\": 'i am',\n",
    " \"i've\": 'i have',\n",
    " \"isn't\": 'is not',\n",
    " \"it'd\": 'it would',\n",
    " \"it'd've\": 'it would have',\n",
    " \"it'll\": 'it will',\n",
    " \"it'll've\": 'it will have',\n",
    " \"it's\": 'it is',\n",
    " \"let's\": 'let us',\n",
    " \"ma'am\": 'madam',\n",
    " \"mayn't\": 'may not',\n",
    " \"might've\": 'might have',\n",
    " \"mightn't\": 'might not',\n",
    " \"mightn't've\": 'might not have',\n",
    " \"must've\": 'must have',\n",
    " \"mustn't\": 'must not',\n",
    " \"mustn't've\": 'must not have',\n",
    " \"needn't\": 'need not',\n",
    " \"needn't've\": 'need not have',\n",
    " \"o'clock\": 'of the clock',\n",
    " \"oughtn't\": 'ought not',\n",
    " \"oughtn't've\": 'ought not have',\n",
    " \"sha'n't\": 'shall not',\n",
    " \"shan't\": 'shall not',\n",
    " \"shan't've\": 'shall not have',\n",
    " \"she'd\": 'she would',\n",
    " \"she'd've\": 'she would have',\n",
    " \"she'll\": 'she will',\n",
    " \"she'll've\": 'she will have',\n",
    " \"she's\": 'she is',\n",
    " \"should've\": 'should have',\n",
    " \"shouldn't\": 'should not',\n",
    " \"shouldn't've\": 'should not have',\n",
    " \"so's\": 'so as',\n",
    " \"so've\": 'so have',\n",
    " \"that'd\": 'that would',\n",
    " \"that'd've\": 'that would have',\n",
    " \"that's\": 'that is',\n",
    " \"there'd\": 'there would',\n",
    " \"there'd've\": 'there would have',\n",
    " \"there's\": 'there is',\n",
    " \"they'd\": 'they would',\n",
    " \"they'd've\": 'they would have',\n",
    " \"they'll\": 'they will',\n",
    " \"they'll've\": 'they will have',\n",
    " \"they're\": 'they are',\n",
    " \"they've\": 'they have',\n",
    " \"this's\": 'this is',\n",
    " \"to've\": 'to have',\n",
    " \"wasn't\": 'was not',\n",
    " \"we'd\": 'we would',\n",
    " \"we'd've\": 'we would have',\n",
    " \"we'll\": 'we will',\n",
    " \"we'll've\": 'we will have',\n",
    " \"we're\": 'we are',\n",
    " \"we've\": 'we have',\n",
    " \"weren't\": 'were not',\n",
    " \"what'll\": 'what will',\n",
    " \"what'll've\": 'what will have',\n",
    " \"what're\": 'what are',\n",
    " \"what's\": 'what is',\n",
    " \"what've\": 'what have',\n",
    " \"when's\": 'when is',\n",
    " \"when've\": 'when have',\n",
    " \"where'd\": 'where did',\n",
    " \"where's\": 'where is',\n",
    " \"where've\": 'where have',\n",
    " \"who'll\": 'who will',\n",
    " \"who'll've\": 'who will have',\n",
    " \"who's\": 'who is',\n",
    " \"who've\": 'who have',\n",
    " \"why's\": 'why is',\n",
    " \"why've\": 'why have',\n",
    " \"will've\": 'will have',\n",
    " \"won't\": 'will not',\n",
    " \"won't've\": 'will not have',\n",
    " \"would've\": 'would have',\n",
    " \"wouldn't\": 'would not',\n",
    " \"wouldn't've\": 'would not have',\n",
    " \"y'all\": 'you all',\n",
    " \"y'all'd\": 'you all would',\n",
    " \"y'all'd've\": 'you all would have',\n",
    " \"y'all're\": 'you all are',\n",
    " \"y'all've\": 'you all have',\n",
    " \"you'd\": 'you would',\n",
    " \"you'd've\": 'you would have',\n",
    " \"you'll\": 'you will',\n",
    " \"you'll've\": 'you will have',\n",
    " \"you're\": 'you are',\n",
    " \"you've\": 'you have'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "df_politics = glob('../origin_data//politics/*.txt')\n",
    "df_sport = glob('../origin_data//sport/*.txt')\n",
    "df_tech = glob('../origin_data//tech/*.txt')\n",
    "df_entertain = glob('../origin_data//entertainment/*.txt')\n",
    "df_business = glob('../origin_data//business/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df_normal, df_abnormal):\n",
    "\n",
    "    # 데이터 랜덤으로 추출\n",
    "    idx = np.random.permutation(len(df_normal))\n",
    "    \n",
    "    # train_data는 df_normal에서 60%로 구성\n",
    "    # test_data는 test_dataset_normal, test_dataset_abnormal로 구성 -> 이상 데이터가 test_data 절반 이상부터 나오게 하기 위함\n",
    "    train_dataset, test_dataset_normal, test_dataset_abnormal = df_normal.iloc[idx[:int(len(df_normal)*0.6)]], df_normal.iloc[idx[int(len(df_normal)*0.6):int(len(df_normal)*0.6)+int(len(df_normal)*0.2)]], df_normal.iloc[idx[int(len(df_normal)*0.6)+int(len(df_normal)*0.2):]]\n",
    "    \n",
    "    # 카테고리 생성\n",
    "    category_tr = [0] * len(train_dataset)\n",
    "    train_dataset['category'] = category_tr\n",
    "\n",
    "    category_te = [0] * len(test_dataset_normal)\n",
    "    test_dataset_normal['category'] = category_te\n",
    "\n",
    "    category_ab_te = [0] * len(test_dataset_abnormal)\n",
    "    test_dataset_abnormal['category'] = category_ab_te\n",
    "\n",
    "    # 비정상 데이터 추출\n",
    "    ab_idx = np.random.permutation(len(df_abnormal))\n",
    "    ab_data = df_abnormal.iloc[ab_idx[:int(len(ab_idx)*0.2)]]\n",
    "\n",
    "    ab_category = [1] * len(ab_data)\n",
    "    ab_data['category'] = ab_category\n",
    "\n",
    "    # test_dataset에서 절반 이상에서 이상치가 나올 수 있도록 만듬\n",
    "    test_dataset_abnormal_ = pd.concat([test_dataset_abnormal, ab_data], axis=0)\n",
    "    test_ab_idx = np.random.permutation(len(test_dataset_abnormal_))\n",
    "    test_dataset_abnormal_fi = test_dataset_abnormal_.iloc[test_ab_idx]\n",
    "    \n",
    "    test_dataset = pd.concat([test_dataset_normal, test_dataset_abnormal_fi], axis=0)\n",
    "    test_dataset = test_dataset.reset_index(drop=True)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def sentence_embedding(sentence):\n",
    "    input_ids = torch.tensor(tokenizer.encode(sentence, add_special_tokens=True)).unsqueeze(0)\n",
    "    # 문장이 너무 길어서 토큰 갯수가 512가 넘을 때 -> 512개 까지만 사용\n",
    "    if len(input_ids[0])>512:\n",
    "        input_ids = input_ids[0][:512]\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze()\n",
    "    return sentence_embedding.detach().numpy()\n",
    "\n",
    "# 문장을 임베딩 한 후 문장들의 임베딩 값을 평균내어 전체 문서의 임베딩 값을 구함\n",
    "def make_vector(docs):\n",
    "    train_docs_vector = []\n",
    "    stop_words = set(stopwords.words('english'))  # 영어 stopwords를 사용할 경우\n",
    "\n",
    "    for sentences in tqdm(docs):\n",
    "        sentence_vector = []\n",
    "        for sentence in sentences.split('. '):\n",
    "            # stopwords를 제거한 후에 sentence_embedding 수행\n",
    "            sentence_clean = ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n",
    "            if sentence_clean.strip() != '':\n",
    "                sentence_vector.append(sentence_embedding(sentence_clean))\n",
    "            else:\n",
    "                sentence_vector.append(sentence_embedding(sentence))\n",
    "            \n",
    "        train_docs_vector.append(sentence_vector)\n",
    "\n",
    "    docs_embedding = np.array([np.mean(train_docs_vector[idx], axis=0) for idx in range(len(train_docs_vector))])\n",
    "    return docs_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비정상 데이터인 Business를 제외한 정상 데이터\n",
    "normal_dataset = df_politics+df_sport+df_tech+df_entertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1715/1715 [00:00<00:00, 9122.15it/s]\n",
      "100%|██████████| 510/510 [00:00<00:00, 4138.64it/s]\n"
     ]
    }
   ],
   "source": [
    "txt = []\n",
    "\n",
    "for lst in tqdm(normal_dataset):\n",
    "    file = open(lst, 'r')\n",
    "    data = file.read()\n",
    "    data = data.lower()\n",
    "    data = data.strip()\n",
    "    data = re.compile('<.*?>').sub('', data) # <parsing> 제거\n",
    "    data = re.sub('\\s+', ' ', data)  \n",
    "    data = ' '.join([contractions[t] if t in contractions else t for t in data.split(\" \")]) # 약어 정규화\n",
    "    data = re.sub(r\"'s\\b\",\"\",data) # 소유격 제거. Ex) roland's -> roland\n",
    "    \n",
    "    data = data.replace('\\n\\n', '\\n')\n",
    "    data = data.replace('\\n', '. ')\n",
    "    data = data.replace('..', '.')\n",
    "\n",
    "    txt.append(data)\n",
    "\n",
    "txt_2 = []\n",
    "\n",
    "for abnormal in tqdm(df_business):\n",
    "    file = open(abnormal, 'r')\n",
    "    data_ = file.read()\n",
    "    data_ = data_.lower()\n",
    "    data_ = data_.strip()\n",
    "    data_ = re.compile('<.*?>').sub('', data_) # <parsing> 제거\n",
    "    data_ = re.sub('\\s+', ' ', data_)  \n",
    "    data_ = ' '.join([contractions[t] if t in contractions else t for t in data_.split(\" \")]) # 약어 정규화\n",
    "    data_ = re.sub(r\"'s\\b\",\"\",data_) # 소유격 제거. Ex) roland's -> roland\n",
    "    \n",
    "    data_ = data_.replace('\\n\\n', '\\n')\n",
    "    data_ = data_.replace('\\n', '. ')\n",
    "    data_ = data_.replace('..', '.')\n",
    "\n",
    "    txt_2.append(data_)\n",
    "\n",
    "df_normal = pd.DataFrame(txt, columns=['origin'])\n",
    "df_normal = df_normal.reset_index(drop=True)\n",
    "df_abnormal = pd.DataFrame(txt_2, columns=['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1028/1028 [05:33<00:00,  3.09it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 788/788 [04:10<00:00,  3.15it/s]\n",
      " 10%|█         | 1/10 [10:26<1:33:56, 626.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 390\n",
      "Delay: 59\n",
      "Group 1 proportion: 0.125\n",
      "Group 2 proportion: 0.312\n",
      "t-statistic: -2.927\n",
      "p-value: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [05:34<00:00,  3.07it/s]\n",
      "100%|██████████| 788/788 [04:05<00:00,  3.21it/s]\n",
      " 20%|██        | 2/10 [20:51<1:23:24, 625.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 453\n",
      "Delay: 95\n",
      "Group 1 proportion: 0.087\n",
      "Group 2 proportion: 0.263\n",
      "t-statistic: -2.975\n",
      "p-value: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [05:26<00:00,  3.15it/s]\n",
      "100%|██████████| 788/788 [04:13<00:00,  3.11it/s]\n",
      " 30%|███       | 3/10 [31:16<1:12:57, 625.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 469\n",
      "Delay: 103\n",
      "Group 1 proportion: 0.113\n",
      "Group 2 proportion: 0.300\n",
      "t-statistic: -2.994\n",
      "p-value: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [05:30<00:00,  3.11it/s]\n",
      "100%|██████████| 788/788 [04:08<00:00,  3.17it/s]\n",
      " 40%|████      | 4/10 [41:39<1:02:27, 624.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 426\n",
      "Delay: 80\n",
      "Group 1 proportion: 0.113\n",
      "Group 2 proportion: 0.300\n",
      "t-statistic: -2.994\n",
      "p-value: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [05:28<00:00,  3.13it/s]\n",
      "100%|██████████| 788/788 [04:10<00:00,  3.15it/s]\n",
      " 50%|█████     | 5/10 [52:02<51:58, 623.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 359\n",
      "Delay: 47\n",
      "Group 1 proportion: 0.075\n",
      "Group 2 proportion: 0.237\n",
      "t-statistic: -2.886\n",
      "p-value: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [05:26<00:00,  3.15it/s]\n",
      "100%|██████████| 788/788 [04:09<00:00,  3.15it/s]\n",
      " 60%|██████    | 6/10 [1:02:22<41:31, 622.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 354\n",
      "Delay: 42\n",
      "Group 1 proportion: 0.087\n",
      "Group 2 proportion: 0.263\n",
      "t-statistic: -2.975\n",
      "p-value: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [05:25<00:00,  3.16it/s]\n",
      "100%|██████████| 788/788 [04:08<00:00,  3.17it/s]\n",
      "100%|██████████| 1028/1028 [05:28<00:00,  3.13it/s]\n",
      "100%|██████████| 788/788 [04:08<00:00,  3.17it/s]\n",
      " 80%|████████  | 8/10 [1:23:01<20:42, 621.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 449\n",
      "Delay: 93\n",
      "Group 1 proportion: 0.013\n",
      "Group 2 proportion: 0.125\n",
      "t-statistic: -2.866\n",
      "p-value: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [05:35<00:00,  3.07it/s]\n",
      "100%|██████████| 788/788 [04:00<00:00,  3.28it/s]\n",
      " 90%|█████████ | 9/10 [1:33:20<10:20, 620.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 437\n",
      "Delay: 86\n",
      "Group 1 proportion: 0.100\n",
      "Group 2 proportion: 0.275\n",
      "t-statistic: -2.891\n",
      "p-value: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [05:25<00:00,  3.16it/s]\n",
      "100%|██████████| 788/788 [04:08<00:00,  3.16it/s]\n",
      "100%|██████████| 10/10 [1:43:38<00:00, 621.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 450\n",
      "Delay: 92\n",
      "Group 1 proportion: 0.075\n",
      "Group 2 proportion: 0.237\n",
      "t-statistic: -2.886\n",
      "p-value: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "clu = [5,10,15,20,25,30,35,40,45,50] # K-means의 클러스터 갯수 \n",
    "threshold = np.arange(0,4.5,0.1) # 데이터와 클러스터의 중앙값으로부터의 거리에서 이상치로 판단할 기준\n",
    "final_acc = []\n",
    "final_precision = []\n",
    "final_recall = []\n",
    "final_f1 = []\n",
    "final_delay = []\n",
    "epoch_time = []\n",
    "\n",
    "for n in tqdm(range(1, 11)):\n",
    "    # 시작 시간\n",
    "    start_time = time.time()\n",
    "    # train_data, test_data만들기\n",
    "    train_dataset, test_dataset = make_dataset(df_normal, df_abnormal)\n",
    "    \n",
    "    # 텍스트 데이타 임베딩\n",
    "    train_docs_embedding = make_vector(train_dataset.origin)\n",
    "    test_docs_embedding = make_vector(test_dataset.origin)\n",
    "\n",
    "    best_score = 0\n",
    "    # 모델이 3개 임으로 \n",
    "    i = len(train_dataset)//3\n",
    "    \n",
    "    for c in clu:\n",
    "        \n",
    "        kmeans1 = KMeans(n_clusters=c, random_state=42)\n",
    "        kmeans1.fit(train_docs_embedding[:i])\n",
    "\n",
    "        kmeans2 = KMeans(n_clusters=c, random_state=42)\n",
    "        kmeans2.fit(train_docs_embedding[i:2*i])\n",
    "\n",
    "        kmeans3 = KMeans(n_clusters=c, random_state=42)\n",
    "        kmeans3.fit(train_docs_embedding[2*i:])\n",
    "\n",
    "        # 데이터와 클러스터 중심값으로부터 거리\n",
    "        distances1 = np.zeros(test_docs_embedding.shape[0])\n",
    "        distances2 = np.zeros(test_docs_embedding.shape[0])\n",
    "        distances3 = np.zeros(test_docs_embedding.shape[0])\n",
    "        \n",
    "\n",
    "        for t in threshold:\n",
    "            predictions1 = kmeans1.predict(test_docs_embedding)\n",
    "            predictions2 = kmeans2.predict(test_docs_embedding)\n",
    "            predictions3 = kmeans3.predict(test_docs_embedding)\n",
    "        \n",
    "            # 데이터와 중앙값 거리 구함\n",
    "            for idx in range(test_docs_embedding.shape[0]):\n",
    "                distances1[idx] = np.linalg.norm(test_docs_embedding[idx] - kmeans1.cluster_centers_[predictions1[idx]])\n",
    "                distances2[idx] = np.linalg.norm(test_docs_embedding[idx] - kmeans2.cluster_centers_[predictions2[idx]])\n",
    "                distances3[idx] = np.linalg.norm(test_docs_embedding[idx] - kmeans3.cluster_centers_[predictions3[idx]])\n",
    "        \n",
    "            # 어느 거리부터 이상치로 정할지 나타내는 t를 통해 predict_ensembel구함\n",
    "            # predict_ensembel -> [0,0,2,3,1,2,3,.....], 길이는 test_docs_embedding 갯수 만큼 \n",
    "            predict_ensembel = (distances1>t) * 1 + (distances2>t) * 1 + (distances3>t) * 1 \n",
    "        \n",
    "            # 2개 이상 모델에서 이상치 판단 했을 때 실제 이상치로 판단\n",
    "            predict = np.where(predict_ensembel>=2, 1, 0)\n",
    "            \n",
    "            acc_scores = accuracy_score(test_dataset.category, predict)\n",
    "            f1_s = f1_score(test_dataset.category, predict)\n",
    "            \n",
    "            # f1스코어가 가장 큰 클러스터 갯수(c), 거리 기준값(t)를 구함\n",
    "            if f1_s>best_score:\n",
    "                best_score = f1_s\n",
    "                best_params = {best_score:[c,t]}\n",
    "                whole_window_ensemble = predict_ensembel\n",
    "                whole_window = predict\n",
    "\n",
    "\n",
    "\n",
    "    ###########################################################################################\n",
    "    ######################################여기서 부터 추론######################################\n",
    "    ###########################################################################################    \n",
    "    \n",
    "    # f1스코어가 최대가 되는 kmeans모델\n",
    "    kmeans1 = KMeans(n_clusters=best_params[best_score][0], random_state=42)\n",
    "    kmeans1.fit(train_docs_embedding[:i])\n",
    "\n",
    "    kmeans2 = KMeans(n_clusters=best_params[best_score][0], random_state=42)\n",
    "    kmeans2.fit(train_docs_embedding[i:2*i])\n",
    "\n",
    "    kmeans3 = KMeans(n_clusters=best_params[best_score][0], random_state=42)\n",
    "    kmeans3.fit(train_docs_embedding[2*i:])\n",
    "\n",
    "    distances1 = np.zeros(test_docs_embedding.shape[0])\n",
    "    distances2 = np.zeros(test_docs_embedding.shape[0])\n",
    "    distances3 = np.zeros(test_docs_embedding.shape[0])\n",
    "\n",
    "    predictions1 = kmeans1.predict(test_docs_embedding)\n",
    "    predictions2 = kmeans2.predict(test_docs_embedding)\n",
    "    predictions3 = kmeans3.predict(test_docs_embedding)\n",
    "    \n",
    "    # 그 모델로부터 추론\n",
    "    for i in range(test_docs_embedding.shape[0]):\n",
    "        distances1[i] = np.linalg.norm(test_docs_embedding[i] - kmeans1.cluster_centers_[predictions1[i]])\n",
    "        distances2[i] = np.linalg.norm(test_docs_embedding[i] - kmeans2.cluster_centers_[predictions2[i]])\n",
    "        distances3[i] = np.linalg.norm(test_docs_embedding[i] - kmeans3.cluster_centers_[predictions3[i]])\n",
    "    \n",
    "    # 각각의 모델에서 이상치 여부에 대해 2개 이상의 모델에서 이상치 판단 했을 때 실제 이상치로 판단\n",
    "    predict_ensembel = (distances1>best_params[best_score][1]) * 1 + (distances2>best_params[best_score][1]) * 1 + (distances3>best_params[best_score][1]) * 1 \n",
    "    \n",
    "    predict = np.where(predict_ensembel>=2, 1, 0)\n",
    "\n",
    "    test_acc_scores = accuracy_score(test_dataset.category, predict)\n",
    "    test_pre_scores = precision_score(test_dataset.category, predict)\n",
    "    test_rec_scores = recall_score(test_dataset.category, predict)\n",
    "    test_f1_scores = f1_score(test_dataset.category, predict)\n",
    "    final_acc.append(test_acc_scores)\n",
    "    final_precision.append(test_pre_scores)\n",
    "    final_recall.append(test_rec_scores)\n",
    "    final_f1.append(test_f1_scores)\n",
    "\n",
    "    # 윈도우 사이즈\n",
    "    window_size = 80\n",
    "    ref_window = predict[:window_size] # 레퍼런스 윈도우는 예상한 값의 처음부터 window size까지\n",
    "    ref_ratio = np.count_nonzero(ref_window) / len(ref_window)\n",
    "\n",
    "    first_ab_idx = test_dataset[test_dataset.category==1].index[0]-window_size  # 실제 처음 이상치가 들어간 시점(index)\n",
    "    for delay in range(len(predict) - first_ab_idx):\n",
    "        compare_window = predict[first_ab_idx:first_ab_idx+window_size]  # 실제 처음 이상치가 들어간 시점 부터 윈도우 사이즈 만큼 compare_window생성\n",
    "        compare_ratio = np.count_nonzero(compare_window) / len(compare_window)  # 1과 0 비율 구함\n",
    "        first_ab_idx+=1   # 1씩 움지이면서 ref_winodw와 compare_window의 1비율을 보며 비교 -> ttest의 p-value값이 0.05보다 작아지면 실제로 새로운 토픽을 찾은거임\n",
    "        t, p = ttest_ind(ref_window, compare_window)\n",
    "        if p<=0.005:\n",
    "            print('몇 번째인지:', delay+first_ab_idx)\n",
    "            print('Delay:', delay)\n",
    "            print(f\"Group 1 proportion: {ref_ratio:.3f}\")\n",
    "            print(f\"Group 2 proportion: {compare_ratio:.3f}\")\n",
    "            print(f\"t-statistic: {t:.3f}\")\n",
    "            print(f\"p-value: {p:.3f}\")\n",
    "            final_delay.append(delay)\n",
    "            break\n",
    "    if len(final_delay) != n:\n",
    "        final_delay.append('none')\n",
    "        \n",
    "    epoch_time.append(round(time.time() - start_time, 1)) # 최종 걸린 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 time: [626.3, 624.9, 625.1, 623.3, 622.4, 621.0, 617.4, 621.1, 618.9, 618.1]\n",
      "평균 621.85\n"
     ]
    }
   ],
   "source": [
    "print('각10번 time:', epoch_time)\n",
    "print('평균', np.mean(epoch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 delay: [59, 95, 103, 80, 47, 42, 'none', 93, 86, 92]\n"
     ]
    }
   ],
   "source": [
    "print('각10번 delay:', final_delay)\n",
    "# print('평균', np.mean(final_delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.44444444444444"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([59, 95, 103, 80, 47, 42, 93, 86, 92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 acc: [0.8629441624365483, 0.8743654822335025, 0.8857868020304569, 0.8527918781725888, 0.8781725888324873, 0.8654822335025381, 0.8984771573604061, 0.9035532994923858, 0.8743654822335025, 0.8857868020304569]\n",
      "평균 0.8781725888324873\n"
     ]
    }
   ],
   "source": [
    "print('각10번 acc:', final_acc)\n",
    "print('평균', np.mean(final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 precision: [0.48125, 0.5103448275862069, 0.5454545454545454, 0.4551282051282051, 0.5238095238095238, 0.48717948717948717, 0.5932203389830508, 0.63, 0.5100671140939598, 0.5508474576271186]\n",
      "평균 0.5287301499862098\n"
     ]
    }
   ],
   "source": [
    "print('각10번 precision:', final_precision)\n",
    "print('평균', np.mean(final_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 f1: [0.5877862595419847, 0.5991902834008097, 0.6153846153846153, 0.5503875968992248, 0.5789473684210527, 0.5891472868217054, 0.6363636363636364, 0.6237623762376239, 0.6055776892430279, 0.5909090909090909]\n",
      "평균 0.5977456203222771\n"
     ]
    }
   ],
   "source": [
    "print('각10번 f1:', final_f1)\n",
    "print('평균', np.mean(final_f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나머지 토픽에 대해서는 이하 동일"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약 데이터(Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytextrank.base.BaseTextRankFactory at 0x7fd0b1432680>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import pytextrank\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from icecream import ic\n",
    "from math import sqrt\n",
    "from operator import itemgetter\n",
    "nlp.add_pipe(\"textrank\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics = glob('../origin_data//politics/*.txt')\n",
    "df_sport = glob('../origin_data//sport/*.txt')\n",
    "df_tech = glob('../origin_data//tech/*.txt')\n",
    "df_entertain = glob('../origin_data//entertainment/*.txt')\n",
    "df_business = glob('../origin_data//business/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 textrank를 활용해 요약해주는 함수\n",
    "def text_r(text):\n",
    "    doc = nlp(text)\n",
    "    sent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\n",
    "    \n",
    "    # 최대 문장의 50% 길이로 텍스트 요약\n",
    "    limit_phrases = len(sent_tokenize(text))//2\n",
    "\n",
    "    phrase_id = 0\n",
    "    unit_vector = []\n",
    "\n",
    "    for p in doc._.phrases:\n",
    "\n",
    "        unit_vector.append(p.rank)\n",
    "\n",
    "        for chunk in p.chunks:\n",
    "\n",
    "            for sent_start, sent_end, sent_vector in sent_bounds:\n",
    "                if chunk.start >= sent_start and chunk.end <= sent_end:\n",
    "                    sent_vector.add(phrase_id)\n",
    "                    break\n",
    "\n",
    "        phrase_id += 1\n",
    "\n",
    "        if phrase_id == limit_phrases:\n",
    "            break\n",
    "\n",
    "    sum_ranks = sum(unit_vector)\n",
    "\n",
    "    unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n",
    "\n",
    "    sent_rank = {}\n",
    "    sent_id = 0\n",
    "\n",
    "    for sent_start, sent_end, sent_vector in sent_bounds:\n",
    "        sum_sq = 0.0\n",
    "        for phrase_id in range(len(unit_vector)):\n",
    "\n",
    "            if phrase_id not in sent_vector:\n",
    "                sum_sq += unit_vector[phrase_id]**2.0\n",
    "\n",
    "        sent_rank[sent_id] = sqrt(sum_sq)\n",
    "        sent_id += 1\n",
    "\n",
    "    sorted(sent_rank.items(), key=itemgetter(1)) \n",
    "\n",
    "    # 50% 요약\n",
    "    limit_sentences = len(sent_tokenize(text))//2\n",
    "\n",
    "    sent_text = {}\n",
    "    sent_id = 0\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_text[sent_id] = sent.text\n",
    "        sent_id += 1\n",
    "\n",
    "    num_sent = 0\n",
    "    sum_text = []\n",
    "        \n",
    "    for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n",
    "        sum_text.append(sent_text[sent_id])\n",
    "        num_sent += 1\n",
    "\n",
    "        if num_sent == limit_sentences:\n",
    "            break\n",
    "    return sum_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비정상 데이터인 Business를 제외한 정상 데이터\n",
    "normal_dataset = df_politics+df_sport+df_tech+df_entertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1715/1715 [00:00<00:00, 9226.34it/s]\n",
      "100%|██████████| 510/510 [00:00<00:00, 10162.84it/s]\n"
     ]
    }
   ],
   "source": [
    "txt = []\n",
    "\n",
    "for lst in tqdm(normal_dataset):\n",
    "    file = open(lst, 'r')\n",
    "    data = file.read()\n",
    "    data = data.lower()\n",
    "    data = data.strip()\n",
    "    data = re.compile('<.*?>').sub('', data) # <parsing> 제거\n",
    "    data = re.sub('\\s+', ' ', data)  \n",
    "    data = ' '.join([contractions[t] if t in contractions else t for t in data.split(\" \")]) # 약어 정규화\n",
    "    data = re.sub(r\"'s\\b\",\"\",data) # 소유격 제거. Ex) roland's -> roland\n",
    "    \n",
    "    data = data.replace('\\n\\n', '\\n')\n",
    "    data = data.replace('\\n', '. ')\n",
    "    data = data.replace('..', '.')\n",
    "\n",
    "    txt.append(data)\n",
    "\n",
    "txt_2 = []\n",
    "\n",
    "for abnormal in tqdm(df_business):\n",
    "    file = open(abnormal, 'r')\n",
    "    data_ = file.read()\n",
    "    data_ = data_.lower()\n",
    "    data_ = data_.strip()\n",
    "    data_ = re.compile('<.*?>').sub('', data_) # <parsing> 제거\n",
    "    data_ = re.sub('\\s+', ' ', data_)  \n",
    "    data_ = ' '.join([contractions[t] if t in contractions else t for t in data_.split(\" \")]) # 약어 정규화\n",
    "    data_ = re.sub(r\"'s\\b\",\"\",data_) # 소유격 제거. Ex) roland's -> roland\n",
    "    \n",
    "    data_ = data_.replace('\\n\\n', '\\n')\n",
    "    data_ = data_.replace('\\n', '. ')\n",
    "    data_ = data_.replace('..', '.')\n",
    "\n",
    "    txt_2.append(data_)\n",
    "\n",
    "df_normal = pd.DataFrame(txt, columns=['origin'])\n",
    "df_normal = df_normal.reset_index(drop=True)\n",
    "df_abnormal = pd.DataFrame(txt_2, columns=['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1029/1029 [02:45<00:00,  6.22it/s]\n",
      "100%|██████████| 788/788 [02:11<00:00,  6.01it/s]\n",
      " 10%|█         | 1/10 [06:47<1:01:10, 407.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 351\n",
      "Delay: 41\n",
      "Group 1 proportion: 0.075\n",
      "Group 2 proportion: 0.237\n",
      "t-statistic: -2.886\n",
      "p-value: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1029/1029 [02:50<00:00,  6.04it/s]\n",
      "100%|██████████| 788/788 [06:11<00:00,  2.12it/s]\n",
      " 20%|██        | 2/10 [19:09<1:20:33, 604.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 386\n",
      "Delay: 58\n",
      "Group 1 proportion: 0.062\n",
      "Group 2 proportion: 0.225\n",
      "t-statistic: -2.992\n",
      "p-value: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1029/1029 [02:43<00:00,  6.28it/s]\n",
      "100%|██████████| 788/788 [02:00<00:00,  6.53it/s]\n",
      " 30%|███       | 3/10 [25:58<1:00:06, 515.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 1023\n",
      "Delay: 379\n",
      "Group 1 proportion: 0.087\n",
      "Group 2 proportion: 0.263\n",
      "t-statistic: -2.975\n",
      "p-value: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1029/1029 [02:37<00:00,  6.53it/s]\n",
      "100%|██████████| 788/788 [02:15<00:00,  5.83it/s]\n",
      " 40%|████      | 4/10 [32:44<47:12, 472.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 420\n",
      "Delay: 77\n",
      "Group 1 proportion: 0.075\n",
      "Group 2 proportion: 0.237\n",
      "t-statistic: -2.886\n",
      "p-value: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1029/1029 [06:09<00:00,  2.78it/s]\n",
      "100%|██████████| 788/788 [04:52<00:00,  2.70it/s]\n",
      " 50%|█████     | 5/10 [45:38<48:23, 580.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 590\n",
      "Delay: 162\n",
      "Group 1 proportion: 0.037\n",
      "Group 2 proportion: 0.175\n",
      "t-statistic: -2.877\n",
      "p-value: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1029/1029 [02:45<00:00,  6.21it/s]\n",
      "100%|██████████| 788/788 [02:04<00:00,  6.32it/s]\n",
      " 60%|██████    | 6/10 [52:19<34:39, 519.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 1305\n",
      "Delay: 519\n",
      "Group 1 proportion: 0.087\n",
      "Group 2 proportion: 0.667\n",
      "t-statistic: -3.337\n",
      "p-value: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1029/1029 [02:55<00:00,  5.87it/s]\n",
      "100%|██████████| 788/788 [02:15<00:00,  5.81it/s]\n",
      " 70%|███████   | 7/10 [59:27<24:29, 489.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 640\n",
      "Delay: 185\n",
      "Group 1 proportion: 0.050\n",
      "Group 2 proportion: 0.200\n",
      "t-statistic: -2.927\n",
      "p-value: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1029/1029 [03:11<00:00,  5.38it/s]\n",
      "100%|██████████| 788/788 [02:15<00:00,  5.80it/s]\n",
      "100%|██████████| 1029/1029 [02:59<00:00,  5.73it/s]\n",
      "100%|██████████| 788/788 [02:01<00:00,  6.50it/s]\n",
      " 90%|█████████ | 9/10 [1:13:43<07:35, 455.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 957\n",
      "Delay: 342\n",
      "Group 1 proportion: 0.062\n",
      "Group 2 proportion: 0.225\n",
      "t-statistic: -2.992\n",
      "p-value: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1029/1029 [02:37<00:00,  6.55it/s]\n",
      "100%|██████████| 788/788 [01:58<00:00,  6.63it/s]\n",
      "100%|██████████| 10/10 [1:20:10<00:00, 481.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째인지: 447\n",
      "Delay: 91\n",
      "Group 1 proportion: 0.125\n",
      "Group 2 proportion: 0.312\n",
      "t-statistic: -2.927\n",
      "p-value: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "clu = [5,10,15,20,25,30,35,40,45,50]  # K-means의 클러스터 갯수 \n",
    "threshold = np.arange(0,4.5,0.1) # 데이터와 클러스터의 중앙값으로부터의 거리에서 이상치로 판단할 기준\n",
    "final_acc = []\n",
    "final_precision = []\n",
    "final_recall = []\n",
    "final_f1 = []\n",
    "final_delay = []\n",
    "epoch_time = []\n",
    "\n",
    "for n in tqdm(range(1, 11)):\n",
    "    # 시작 시간\n",
    "    start_time = time.time()\n",
    "\n",
    "    # train_data, test_data\n",
    "    train_dataset, test_dataset = make_dataset(df_normal, df_abnormal)\n",
    "    \n",
    "    # train_data의 원본 데이터 요약\n",
    "    train_sum = []\n",
    "    for i in range(len(train_dataset)):\n",
    "        summ1 = text_r(train_dataset.origin.iloc[i])\n",
    "        train_sum.append(' '.join(summ1))\n",
    "    train_dataset['summary'] = train_sum\n",
    "\n",
    "    # test_data의 원본 데이터 요약\n",
    "    test_sum = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        summ2 = text_r(test_dataset.origin.iloc[i])\n",
    "        test_sum.append(' '.join(summ2))\n",
    "    test_dataset['summary'] = test_sum\n",
    "\n",
    "    # 문서 임베딩\n",
    "    train_docs_embedding = make_vector(train_dataset.summary)\n",
    "    test_docs_embedding = make_vector(test_dataset.summary)\n",
    "\n",
    "\n",
    "    best_score = 0\n",
    "    # 모델이 3개 임으로\n",
    "    i = len(train_dataset)//3\n",
    "    \n",
    "    for c in clu:\n",
    "        \n",
    "        kmeans1 = KMeans(n_clusters=c, random_state=42)\n",
    "        kmeans1.fit(train_docs_embedding[:i])\n",
    "\n",
    "        kmeans2 = KMeans(n_clusters=c, random_state=42)\n",
    "        kmeans2.fit(train_docs_embedding[i:2*i])\n",
    "\n",
    "        kmeans3 = KMeans(n_clusters=c, random_state=42)\n",
    "        kmeans3.fit(train_docs_embedding[2*i:])\n",
    "\n",
    "        # 데이터와 클러스터 중심값으로부터 거리\n",
    "        distances1 = np.zeros(test_docs_embedding.shape[0])\n",
    "        distances2 = np.zeros(test_docs_embedding.shape[0])\n",
    "        distances3 = np.zeros(test_docs_embedding.shape[0])\n",
    "        \n",
    "\n",
    "        for t in threshold:\n",
    "            predictions1 = kmeans1.predict(test_docs_embedding)\n",
    "            predictions2 = kmeans2.predict(test_docs_embedding)\n",
    "            predictions3 = kmeans3.predict(test_docs_embedding)\n",
    "        \n",
    "                        \n",
    "            # 데이터와 클러스터의 중심값 거리 구함\n",
    "            for idx in range(test_docs_embedding.shape[0]):\n",
    "                distances1[idx] = np.linalg.norm(test_docs_embedding[idx] - kmeans1.cluster_centers_[predictions1[idx]])\n",
    "                distances2[idx] = np.linalg.norm(test_docs_embedding[idx] - kmeans2.cluster_centers_[predictions2[idx]])\n",
    "                distances3[idx] = np.linalg.norm(test_docs_embedding[idx] - kmeans3.cluster_centers_[predictions3[idx]])\n",
    "                \n",
    "            # 어느 거리부터 이상치로 정할지 나타내는 t를 통해 predict_ensembel구함\n",
    "            # predict_ensembel -> [0,0,2,3,1,2,3,.....], 길이는 test_docs_embedding 갯수 만큼 \n",
    "            predict_ensembel = (distances1>t) * 1 + (distances2>t) * 1 + (distances3>t) * 1 \n",
    "            \n",
    "            \n",
    "            # 2개 이상 모델에서 이상치 판단 했을 때 실제 이상치로 판단\n",
    "            predict = np.where(predict_ensembel>=2, 1, 0)\n",
    "            \n",
    "            acc_scores = accuracy_score(test_dataset.category, predict)\n",
    "            f1_s = f1_score(test_dataset.category, predict)\n",
    "            \n",
    "            \n",
    "            # f1스코어가 가장 큰 클러스터 갯수(c), 거리 기준값(t)를 구함\n",
    "            if f1_s>best_score:\n",
    "                best_score = f1_s\n",
    "                best_params = {best_score:[c,t]}\n",
    "                whole_window_ensemble = predict_ensembel\n",
    "                whole_window = predict\n",
    "\n",
    "    ###########################################################################################\n",
    "    ######################################여기서 부터 추론######################################\n",
    "    ###########################################################################################    \n",
    "    \n",
    "    # f1스코어가 최대가 되는 kmeans모델\n",
    "    kmeans1 = KMeans(n_clusters=best_params[best_score][0], random_state=42)\n",
    "    kmeans1.fit(train_docs_embedding[:i])\n",
    "\n",
    "    kmeans2 = KMeans(n_clusters=best_params[best_score][0], random_state=42)\n",
    "    kmeans2.fit(train_docs_embedding[i:2*i])\n",
    "\n",
    "    kmeans3 = KMeans(n_clusters=best_params[best_score][0], random_state=42)\n",
    "    kmeans3.fit(train_docs_embedding[2*i:])\n",
    "\n",
    "    \n",
    "    distances1 = np.zeros(test_docs_embedding.shape[0])\n",
    "    distances2 = np.zeros(test_docs_embedding.shape[0])\n",
    "    distances3 = np.zeros(test_docs_embedding.shape[0])\n",
    "    \n",
    "    predictions1 = kmeans1.predict(test_docs_embedding)\n",
    "    predictions2 = kmeans2.predict(test_docs_embedding)\n",
    "    predictions3 = kmeans3.predict(test_docs_embedding)\n",
    "    \n",
    "\n",
    "    for i in range(test_docs_embedding.shape[0]):\n",
    "        distances1[i] = np.linalg.norm(test_docs_embedding[i] - kmeans1.cluster_centers_[predictions1[i]])\n",
    "        distances2[i] = np.linalg.norm(test_docs_embedding[i] - kmeans2.cluster_centers_[predictions2[i]])\n",
    "        distances3[i] = np.linalg.norm(test_docs_embedding[i] - kmeans3.cluster_centers_[predictions3[i]])\n",
    "\n",
    "    predict_ensembel = (distances1>best_params[best_score][1]) * 1 + (distances2>best_params[best_score][1]) * 1 + (distances3>best_params[best_score][1]) * 1 \n",
    "    \n",
    "    predict = np.where(predict_ensembel>=2, 1, 0)\n",
    "\n",
    "    test_acc_scores = accuracy_score(test_dataset.category, predict)\n",
    "    test_pre_scores = precision_score(test_dataset.category, predict)\n",
    "    test_rec_scores = recall_score(test_dataset.category, predict)\n",
    "    test_f1_scores = f1_score(test_dataset.category, predict)\n",
    "    final_acc.append(test_acc_scores)\n",
    "    final_precision.append(test_pre_scores)\n",
    "    final_recall.append(test_rec_scores)\n",
    "    final_f1.append(test_f1_scores)\n",
    "\n",
    "    window_size = 80\n",
    "    window_size = 80\n",
    "    ref_window = predict[:window_size] # 레퍼런스 윈도우는 예상한 값의 처음부터 window size까지\n",
    "    ref_ratio = np.count_nonzero(ref_window) / len(ref_window)\n",
    "\n",
    "    first_ab_idx = test_dataset[test_dataset.category==1].index[0]-window_size # 실제 처음 이상치가 들어간 시점(index)\n",
    "    for delay in range(len(predict) - first_ab_idx):\n",
    "        compare_window = predict[first_ab_idx:first_ab_idx+window_size] # 실제 처음 이상치가 들어간 시점 부터 윈도우 사이즈 만큼 compare_window생성\n",
    "        compare_ratio = np.count_nonzero(compare_window) / len(compare_window) # 0과 1비율\n",
    "        first_ab_idx+=1 # 1씩 움지이면서 ref_winodw와 compare_window의 1비율을 보며 비교 -> ttest의 p-value값이 0.05보다 작아지면 실제로 새로운 토픽을 찾은거임\n",
    "\n",
    "        t, p = ttest_ind(ref_window, compare_window)\n",
    "        if p<=0.005:\n",
    "            print('몇 번째인지:', delay+first_ab_idx)\n",
    "            print('Delay:', delay)\n",
    "            print(f\"Group 1 proportion: {ref_ratio:.3f}\")\n",
    "            print(f\"Group 2 proportion: {compare_ratio:.3f}\")\n",
    "            print(f\"t-statistic: {t:.3f}\")\n",
    "            print(f\"p-value: {p:.3f}\")\n",
    "            final_delay.append(delay)\n",
    "            break\n",
    "    if len(final_delay) != n:\n",
    "        final_delay.append('none')\n",
    "        \n",
    "    epoch_time.append(round(time.time() - start_time, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 time: [407.8, 741.5, 409.3, 406.1, 773.2, 401.8, 427.9, 443.5, 412.5, 386.3]\n",
      "평균 480.99000000000007\n"
     ]
    }
   ],
   "source": [
    "print('각10번 time:', epoch_time)\n",
    "print('평균', np.mean(epoch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 delay: [41, 58, 379, 77, 162, 519, 185, 'none', 342, 91]\n"
     ]
    }
   ],
   "source": [
    "print('각10번 delay:', final_delay)\n",
    "# print('평균', np.mean(final_delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 acc: [0.8401015228426396, 0.8642131979695431, 0.8908629441624365, 0.8604060913705583, 0.8908629441624365, 0.8984771573604061, 0.8895939086294417, 0.8807106598984772, 0.8984771573604061, 0.8489847715736041]\n",
      "평균 0.876269035532995\n"
     ]
    }
   ],
   "source": [
    "print('각10번 acc:', final_acc)\n",
    "print('평균', np.mean(final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 precision: [0.42857142857142855, 0.48031496062992124, 0.5769230769230769, 0.4722222222222222, 0.5833333333333334, 0.6145833333333334, 0.5714285714285714, 0.5307692307692308, 0.6195652173913043, 0.445859872611465]\n",
      "평균 0.5323571247213887\n"
     ]
    }
   ],
   "source": [
    "print('각10번 precision:', final_precision)\n",
    "print('평균', np.mean(final_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각10번 f1: [0.5333333333333333, 0.5327510917030568, 0.5825242718446602, 0.5528455284552845, 0.5656565656565657, 0.595959595959596, 0.5797101449275363, 0.5948275862068965, 0.5876288659793814, 0.5405405405405405]\n",
      "평균 0.5665777524606851\n"
     ]
    }
   ],
   "source": [
    "print('각10번 f1:', final_f1)\n",
    "print('평균', np.mean(final_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
